<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

	<link rel="stylesheet" href="./index_files/uikit.min.css">
	<link href="./index_files/style.css" rel="stylesheet">
   </head>

   <body>
    <header>
        


      <div id="logo-area" uk-grid="" class="uk-grid">
  		 <div class="uk-width-3-5 uk-first-column">
     		 <div><a href="https://openVRLab.github.io/index.html" class="uk-logo"> OpenVRLab</a></div>
        <div id="citation">IEEE VR 2022 Workshop - Open Access Tools and Libraries for Virtual Reality<br/>  <br/></div>
      </div>
     
    </div></header>

   
   	<div id="page-container" class="full-viewport uk-grid-row-large uk-grid-collapse uk-grid" uk-grid="">
      <div class="leftcol uk-width-1-5 uk-first-column"></div>
      <div class="middlecol uk-width-4-5">
        <!-- <div id='workshop-title-div'> -->
         <div class="workshop-title">Open Access Tools and Libraries for Virtual Reality</div>
         <div class="workshop-subtitle">IEEE VR Workshop 2022, March 12, 7.00 am New Zealand time / 18.00 pm GMT (Friday 11th of March) / 10.00 am Pacific time (Friday 11th of March) </div>
<!--         </div> -->




	      <br>
 <div id="organizers">
	 <div class="workshop-subsection">Program</div>
          <div class="organizers uk-grid-row-medium uk-grid" uk-grid="">
		  <p class=MsoNormal><b>Data Capture, Analysis and Understanding<span
style='mso-tab-count:1'>         </span>(45 MINS) <span
style='mso-spacerun:yes'> </span><o:p></o:p></b></p>

<p class=MsoNormal><b>7:00 am NZDT </b><a
href="https://www.worldcitytime.com/10am/pst/to/nzdt">https://www.worldcitytime.com/10am/pst/to/nzdt</a></p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal>Rag-Rug: An <span class=GramE>Open Source</span> Toolkit for
Sit<span style='mso-tab-count:1'> </span><span class=SpellE>uated</span>
Visualization and Analytics <b>(15 MINS)</b></p>

<p class=MsoNormal>Dieter <span class=SpellE>Schmalstieg</span>, Philipp Fleck
(Invited from TVCG paper)</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal>Developing Mixed Reality Applications with Platform for
Situated Intelligence <b>(15 MINS)</b></p>

<p class=MsoNormal>Sean Andrist, Dan Bohus, Ashley Feniello, Nick Saw</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal>STAG: A Tool for <span class=SpellE>realtime</span> Replay
and Analysis of Spatial Trajectory and Gaze Information captured in Immersive
Environments <b>(7 MINS)</b></p>

<p class=MsoNormal><span class=SpellE>Aryabrata</span> Basu</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal>Excite-O-Meter: <span class=GramE>an</span> Open-Source
Unity Plugin to Analyze Heart Activity and Movement Trajectories in Custom VR
Environments <b>(7 MINS)</b></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'>Luis Quintero, <span
class=SpellE>Panagiotis</span> <span class=SpellE>Papapetrou</span>, John
Edison Muñoz Cardona, <span class=SpellE>Jeroen</span> de <span class=SpellE>De</span>
<span class=SpellE>mooij</span>, Michael <span class=SpellE>Gaebler</span><o:p></o:p></span></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'><o:p>&nbsp;</o:p></span></p>

<p class=MsoNormal><b>PANEL</b> on data capture and situated intelligence <b>(15
min)<o:p></o:p></b></p>

<p class=MsoNormal>The presenters of this session will be moderated by Dr. Mar
Gonzalez Franco and discuss problems such as standardization and best
practices.</p>

<p class=MsoNormal><b><o:p>&nbsp;</o:p></b></p>

<p class=MsoNormal><b>Break (6 mins)<o:p></o:p></b></p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal><b>Perception and Cognition<o:p></o:p></b></p>

<p class=MsoNormal><b>8:05 am NZDT </b><a
href="https://www.worldcitytime.com/11am/pst/to/nzdt">https://www.worldcitytime.com/11am/pst/to/nzdt</a></p>

<p class=MsoNormal><b><o:p>&nbsp;</o:p></b></p>

<p class=MsoNormal>An Open Platform for Research about Cognitive Load in
Virtual Reality <b>(10 MIN)</b></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'>Olivier <span
class=SpellE>Augereau</span>, Gabriel <span class=SpellE>Brocheton</span>, Pedro
Paulo Do Prado Neto<o:p></o:p></span></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'><o:p>&nbsp;</o:p></span></p>

<p class=MsoNormal>Human Vision vs. Computer Vision: A Readability Study in a
Virtual Reality Environment <b>(5 MIN)</b></p>

<p class=MsoNormal>Zhu Qing, Praveen Edara</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal>Asymmetric Normalization in Social Virtual Reality Studies <b>(10
MIN)</b></p>

<p class=MsoNormal>Jonas <span class=SpellE>Deuchler</span>, Daniel Hepperle, Matthias
Wölfel</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal><b><o:p>&nbsp;</o:p></b></p>

<p class=MsoNormal><b>Authoring and Access<o:p></o:p></b></p>

<p class=MsoNormal><b>8:30 am NZDT </b></p>

<p class=MsoNormal><b><o:p>&nbsp;</o:p></b></p>

<p class=MsoNormal><span class=SpellE>BabiaXR</span>: Virtual Reality software
data visualizations for the Web <b>(10 MIN)</b></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'>David Moreno-Lumbreras,
<span class=SpellE>Jesus</span> M. Gonzalez-Barahona, Andrea Villaverde<o:p></o:p></span></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'><o:p>&nbsp;</o:p></span></p>

<p class=MsoNormal><span class=SpellE>RealityFlow</span>: Open-Source
Multi-User Immersive Authoring <b>(10 MIN)</b></p>

<p class=MsoNormal>John T Murray</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal>NUI-<span class=SpellE>SpatialMarkers</span>: AR Spatial
Markers <span class=GramE>For</span> the Rest of Us <b>(10 MIN)</b></p>

<p class=MsoNormal>Alex G <span class=SpellE>Karduna</span>, Adam Sinclair
Williams, Francisco Raul Ortega</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal><b>Break (5 mins)<o:p></o:p></b></p>

<p class=MsoNormal><b><o:p>&nbsp;</o:p></b></p>

<p class=MsoNormal><b>Avatar Tools<o:p></o:p></b></p>

<p class=MsoNormal><b>9:05 am NZDT </b><a
href="https://www.worldcitytime.com/12am/pst/to/nzdt">https://www.worldcitytime.com/12am/pst/to/nzdt</a></p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal>Physics-based character animation for Virtual Reality <b>(10
MIN)</b></p>

<p class=MsoNormal>Joan <span class=SpellE>Llobera</span>, Caecilia Charbonnier</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal><span class=SpellE>HeadBox</span>: A Facial <span
class=SpellE>Blendshape</span> Animation Toolkit for the Microsoft <span
class=SpellE>Rocketbox</span> Library <b>(10 MIN)</b></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'>Matias Volonte, Eyal
Ofek, Ken Jakubzak, Shawn Bruner, Mar Gonzalez-Franco<o:p></o:p></span></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'><o:p>&nbsp;</o:p></span></p>

<p class=MsoNormal>Integrating <span class=SpellE>Rocketbox</span> Avatars with
the <span class=SpellE>Ubiq</span> Social VR platform <b>(10 MIN)</b></p>

<p class=MsoNormal>Lisa <span class=SpellE>Izzouzi</span>, Anthony Steed</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal><b>9:35 am NZDT</b></p>

<p class=MsoNormal>Open discussion with all participants on <a
href="https://www.gather.town/">https://www.gather.town/</a> on how to create,
share and centralize better research tools.</p>

<p class=MsoNormal><b>9:55 am NZDT<o:p></o:p></b></p>

<p class=MsoNormal><b>Closing notes and awards (also on <span class=SpellE>Gather.town</span>)<o:p></o:p></b></p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

		  <p class=MsoNormal><b>Data Capture, Analysis and Understanding<span
style='mso-tab-count:1'>         </span>(45 MINS) <span
style='mso-spacerun:yes'> </span><o:p></o:p></b></p>

<p class=MsoNormal><b>7:00 am NZDT </b><a
href="https://www.worldcitytime.com/10am/pst/to/nzdt">https://www.worldcitytime.com/10am/pst/to/nzdt</a></p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal>Rag-Rug: An <span class=GramE>Open Source</span> Toolkit for
Sit<span style='mso-tab-count:1'> </span><span class=SpellE>uated</span>
Visualization and Analytics <b>(15 MINS)</b></p>

<p class=MsoNormal>Dieter <span class=SpellE>Schmalstieg</span>, Philipp Fleck
(Invited from TVCG paper)</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal>Developing Mixed Reality Applications with Platform for
Situated Intelligence <b>(15 MINS)</b></p>

<p class=MsoNormal>Sean Andrist, Dan Bohus, Ashley Feniello, Nick Saw</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal>STAG: A Tool for <span class=SpellE>realtime</span> Replay
and Analysis of Spatial Trajectory and Gaze Information captured in Immersive
Environments <b>(7 MINS)</b></p>

<p class=MsoNormal><span class=SpellE>Aryabrata</span> Basu</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal>Excite-O-Meter: <span class=GramE>an</span> Open-Source
Unity Plugin to Analyze Heart Activity and Movement Trajectories in Custom VR
Environments <b>(7 MINS)</b></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'>Luis Quintero, <span
class=SpellE>Panagiotis</span> <span class=SpellE>Papapetrou</span>, John
Edison Muñoz Cardona, <span class=SpellE>Jeroen</span> de <span class=SpellE>De</span>
<span class=SpellE>mooij</span>, Michael <span class=SpellE>Gaebler</span><o:p></o:p></span></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'><o:p>&nbsp;</o:p></span></p>

<p class=MsoNormal><b>PANEL</b> on data capture and situated intelligence <b>(15
min)<o:p></o:p></b></p>

<p class=MsoNormal>The presenters of this session will be moderated by Dr. Mar
Gonzalez Franco and discuss problems such as standardization and best
practices.</p>

<p class=MsoNormal><b><o:p>&nbsp;</o:p></b></p>

<p class=MsoNormal><b>Break (6 mins)<o:p></o:p></b></p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal><b>Perception and Cognition<o:p></o:p></b></p>

<p class=MsoNormal><b>8:05 am NZDT </b><a
href="https://www.worldcitytime.com/11am/pst/to/nzdt">https://www.worldcitytime.com/11am/pst/to/nzdt</a></p>

<p class=MsoNormal><b><o:p>&nbsp;</o:p></b></p>

<p class=MsoNormal>An Open Platform for Research about Cognitive Load in
Virtual Reality <b>(10 MIN)</b></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'>Olivier <span
class=SpellE>Augereau</span>, Gabriel <span class=SpellE>Brocheton</span>, Pedro
Paulo Do Prado Neto<o:p></o:p></span></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'><o:p>&nbsp;</o:p></span></p>

<p class=MsoNormal>Human Vision vs. Computer Vision: A Readability Study in a
Virtual Reality Environment <b>(5 MIN)</b></p>

<p class=MsoNormal>Zhu Qing, Praveen Edara</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal>Asymmetric Normalization in Social Virtual Reality Studies <b>(10
MIN)</b></p>

<p class=MsoNormal>Jonas <span class=SpellE>Deuchler</span>, Daniel Hepperle, Matthias
Wölfel</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal><b><o:p>&nbsp;</o:p></b></p>

<p class=MsoNormal><b>Authoring and Access<o:p></o:p></b></p>

<p class=MsoNormal><b>8:30 am NZDT </b></p>

<p class=MsoNormal><b><o:p>&nbsp;</o:p></b></p>

<p class=MsoNormal><span class=SpellE>BabiaXR</span>: Virtual Reality software
data visualizations for the Web <b>(10 MIN)</b></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'>David Moreno-Lumbreras,
<span class=SpellE>Jesus</span> M. Gonzalez-Barahona, Andrea Villaverde<o:p></o:p></span></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'><o:p>&nbsp;</o:p></span></p>

<p class=MsoNormal><span class=SpellE>RealityFlow</span>: Open-Source
Multi-User Immersive Authoring <b>(10 MIN)</b></p>

<p class=MsoNormal>John T Murray</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal>NUI-<span class=SpellE>SpatialMarkers</span>: AR Spatial
Markers <span class=GramE>For</span> the Rest of Us <b>(10 MIN)</b></p>

<p class=MsoNormal>Alex G <span class=SpellE>Karduna</span>, Adam Sinclair
Williams, Francisco Raul Ortega</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal><b>Break (5 mins)<o:p></o:p></b></p>

<p class=MsoNormal><b><o:p>&nbsp;</o:p></b></p>

<p class=MsoNormal><b>Avatar Tools<o:p></o:p></b></p>

<p class=MsoNormal><b>9:05 am NZDT </b><a
href="https://www.worldcitytime.com/12am/pst/to/nzdt">https://www.worldcitytime.com/12am/pst/to/nzdt</a></p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal>Physics-based character animation for Virtual Reality <b>(10
MIN)</b></p>

<p class=MsoNormal>Joan <span class=SpellE>Llobera</span>, Caecilia Charbonnier</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal><span class=SpellE>HeadBox</span>: A Facial <span
class=SpellE>Blendshape</span> Animation Toolkit for the Microsoft <span
class=SpellE>Rocketbox</span> Library <b>(10 MIN)</b></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'>Matias Volonte, Eyal
Ofek, Ken Jakubzak, Shawn Bruner, Mar Gonzalez-Franco<o:p></o:p></span></p>

<p class=MsoNormal><span lang=ES style='mso-ansi-language:ES'><o:p>&nbsp;</o:p></span></p>

<p class=MsoNormal>Integrating <span class=SpellE>Rocketbox</span> Avatars with
the <span class=SpellE>Ubiq</span> Social VR platform <b>(10 MIN)</b></p>

<p class=MsoNormal>Lisa <span class=SpellE>Izzouzi</span>, Anthony Steed</p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

<p class=MsoNormal><b>9:35 am NZDT</b></p>

<p class=MsoNormal>Open discussion with all participants on <a
href="https://www.gather.town/">https://www.gather.town/</a> on how to create,
share and centralize better research tools.</p>

<p class=MsoNormal><b>9:55 am NZDT<o:p></o:p></b></p>

<p class=MsoNormal><b>Closing notes and awards (also on <span class=SpellE>Gather.town</span>)<o:p></o:p></b></p>

<p class=MsoNormal><o:p>&nbsp;</o:p></p>

		  </div>
	      </div>
	         <div id="organizers">
           <div class="workshop-subsection">Workshop organizers</div>
          <div class="organizers uk-grid-row-medium uk-grid" uk-grid="">
            <div class="leftcol uk-width-1-4 uk-first-column"><img src="./index_files/margonzalezfranco.jpeg"></div>
            <div class="middlecol uk-width-3-4">
              <h5>Mar Gonzalez-Franco, Microsoft Research<br/> margon@microsoft.com @twi_mar</h5>
              <p>Dr. Mar Gonzalez-Franco is a Principal Researcher in the EPIC - Extended Perception, Interaction and Cognition group. She explores human behaviour and perception to build better technologies in the wild. With a particular focus on spatial computing, avatars and haptics. She is a strong believer in open platforms and public libraries. To democratize the field she has been behind the release of libraries such as the Microsoft Rocketbox avatar library and its sister animation toolkit the Movebox. She has also had a m prolific scientific output (<a href="https://margonzalezfranco.github.io/">website</a>), and her work has transferred to products used daily around the world, like Hololens, Microsoft Soundscape and Together mode in Microsoft Teams. Mar is a Computer Scientist and also holds a PhD in Immersive Virtual Reality and Clinical Psychology from Universitat de Barcelona. Before joining Microsoft she was at Airbus, the MIT, and University College London.</p>
            </div>
         <div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img src="./index_files/matias.jpeg"></div>
            <div class="middlecol uk-width-3-4 uk-grid-margin">
              <h5>Matias Volonte, Northeastern University<br/> m.volonte@northeastern.edu</h5>
              <p>Dr. Matias Volonte is currently a Postdoctoral Research Associate at the Department of Computer Science at Northeastern University. He is working with Dr. Tim Bickmore from the Relational Agents Group.
			  Currently, Matias is investigating the use of virtual agents in which human-agent relationships improve task outcomes in the healthcare domain. Matias is interested in researching simulated face-to-face conversations with an emphasis on the relational and conversational aspects of these interactions. In his Ph.D. at Clemson University, Matias studied human and virtual human interaction in immersive and non-immersive environments with Dr. Sabarish Babu.
			  </p>
            </div>
            <div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img src="./index_files/asteed.jpeg"></div>
            <div class="middlecol uk-width-3-4 uk-grid-margin">
              <h5>Anthony Steed, University College London <br/>a.steed@ucl.ac.uk @anthony_steed</h5>
              <p>Prof. Anthony Steed is Head of the Virtual Environments and Computer Graphics (VECG) group at University College London. Prof. Steed’s research interests extend from virtual reality systems, through to mobile mixed-reality systems, and from system development through to measures of user response to virtual content. He has worked extensively on systems for collaborative mixed reality. He is lead author of a review “Networked Graphics: Building Networked Graphics and Networked Games”. He was the recipient of the IEEE VGTC’s 2016 Virtual Reality Technical Achievement Award.</p>
            </div>
			
			 <div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img src="./index_files/eric.jpeg"></div>
            <div class="middlecol uk-width-3-4 uk-grid-margin">
              <h5>Cheng Yao(Eric) Wang, Cornell University <br/>cw776@cornell.edu @ericwang0701</h5>
              <p>Cheng Yao (Eric) Wang is a Ph.D. candidate in Information Science at Cornell University advised by Prof. Andrea Stevenson Won. His research interests lie at the intersection of Human-Computer Interaction (HCI), Computer-Supported Cooperative Work (CSCW), and Virtual/Mixed Reality (VR/MR). 
			  Previously, he was a research intern at Autodesk, Microsoft Research, and Facebook Reality Lab. In his Ph.D. research, he has implemented and utilized the unique capabilities of 3D reconstruction techniques in different research areas such as reliving experiences, customizing and animating avatars in VR, and adaptive AR/MR user interface, as well as identifying its privacy concerns and challenges. 
</p>
            </div>
			
          </div>





	      <br>
	      <br>



        <div class="workshop-subsection">Overview</div>
        <p>Virtual Reality technologies are growing fast, and so is the VR research community. In this scenario, it is more important than ever that academic research builds upon best practices and results to amplify impact in the broader field. Open-Source tools are one means to propagate best practice as they lower the barrier to entry for researchers from an engineering point of view, while also embodying the prior work on which the tools were built. Open access tools are also critical to eliminate redundancies and increase world research collaboration in VR. At a time that academic research is growing it also needs to move as fast as the industry, collaboration and shared tools is the best way to do it.</p>

         <div class="workshop-subsection">Description</div>
         <p>Driven by uptake in consumer and professional markets, virtual reality technologies are now being developed at a significant pace. Academic research needs to build upon best practice and results if it is to generate generalizable results that have impact in the broader field. Open-Source tools are one means to propagate best practice as they lower the barrier to entry for researchers from an engineering point of view, while also embodying the prior work on which the tools were built. Open access tools are also critical to eliminate redundancies and increase world research collaboration in VR. At a time that academic research needs to move as fast as the industry, collaboration and shared tools is the best way to do it.   </p>

		<p> 
		 In this workshop, we will gather with creators and users of open-access libraries ranging  from avatars (like the Microsoft Rocketbox avatar library) to animation to networking  (Ubiq toolkit) to explore how open access tools are helping advance the VR community.  </p>

		<p>We invite all researchers to join this workshop and learn from existing libraries. We also invite submissions on technical and systems papers that describe new libraries and or new features on existing libraries. </p>


		<p>In this workshop, we will explore these open-access tools, their accessibility, and what type of applications they enable. Finally, we invite users of current libraries to submit their papers and share their learnings while using these tools. </p>


		<p>All papers and repos will be collected and curated in the workshop website  <a href="https://openVRlab.github.io/">https://openVRlab.GitHub.io</a>.</p>

	<div id="callforpapers">

		  <div class="workshop-subsection">Workshop topics</div>
          <ul>
            <li>Open source libraries and tools (from avatars to AI to tracking to networking)</li>
            <li>Research libraries and tools</li>
            <li>New tools, new features, new repos</li>
            <li>Usage of libraries/tools</li>
          </ul>
         <div class="workshop-subsection">Format and submission guidelines</div>
        <p>Submissions should include a title, a list of authors, and be 2-4 pages. All paper submissions must be in English. All IEEE VR Conference Paper submissions must be prepared in IEEE Computer Society VGTC format (<a href="https://tc.computer.org/vgtc/publications/conference/"> https://tc.computer.org/vgtc/publications/conference/</a>) and submitted in PDF format. 
Accepting work of Research papers, Technical notes, Position papers, Work-in-progress papers. The accepted papers will be featured in the IEEE Xplore library. Additional pages can be considered on a case by case basis, but you should check with the workshop organizers (<a href="mailto:margon@microsoft.com">margon@microsoft.com</a>) before the submission deadline. Acceptable paper types are work-in-progress, research papers, position papers, or commentaries. Submissions will be reviewed by the organisers and accepted submissions will give a 10-minute talk with a panel discussion at the end of the session. At least one author must register for the workshop. Selected submissions will get the opportunity to be extended to articles to be considered for a special issue.</p>
	      <p>To submit your work, visit <a href="https://new.precisionconference.com/vr">https://new.precisionconference.com/vr</a></p>
The organizers will review all the submissions.
        <div class="workshop-subsection">Important dates</div>
        <ul>
          <li>Submission deadline: January 17</li>
          <li>Notification of acceptance: January 20</li>
          <li>Camera-ready deadline: January 29</li>
          <li>Workshop: March 12</li>
        </ul>
		</div>


<br>



 <div class="workshop-subsection">Tentative agenda for the workshop</div> The workshop will be about 3 hours, but please book the full time slot from 7:00-10:00 am New Zealand) on your calendar.<p></p>
      <ul>
	    <li><strong>7:00-7:10</strong>: Introduction by the organizers (will be recorded)</li>
            <li><strong>7:15-8:30</strong>: Short presentations of the accepted papers: 5 min per submission, 2 min talk + questions</li>
	    <li><strong>8:30-8:45</strong>: Short break and joining breakout rooms</li>
	    <li><strong>8:45-9:15</strong>: Discussion in the breakout rooms (using MIRO board)</li>
            <li><strong>9:15-9:45</strong>: Presentations of the four breakout rooms and general discussion</li>
	    <li><strong>9:45-10:00</strong>: Closing and call for getting involved in Open Sourcing efforts</li>
         </ul>

	<br>

      <p>We will be using a MIRO board for the discussions.</p>
    
    	<br><br>








        </div>
		<br>
  <div class="rightcol uk-width-1-5"></div>
    </div>
    </div>


    <div id="footer-area" class="uk-grid-collapse uk-grid uk-grid-margin uk-first-column" uk-grid="">
          <div class="leftcol uk-width-1-5 uk-first-column"></div>
          <div class="middlecol uk-width-3-5">For questions and comments, please contact <a href="mailto:margon@microsoft.com">margon@microsoft.com</a>.</div>
          <div class="rightcol uk-width-1-5"></div>
    </div>

   

</div></body></html>
