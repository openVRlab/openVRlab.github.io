<!DOCTYPE html>
<!-- saved from url=(0028)https://openvrlab.github.io/ -->
<html lang="en"><head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta property="og:image" content="index_files/urlIcon.png"/>
	<link rel="stylesheet" href="index_files/uikit.min.css">
	<link href="index_files/style.css" rel="stylesheet">
   <style>undefined</style><link rel="preconnect" href="https://fonts.googleapis.com/" crossorigin="true"><link rel="preconnect" href="https://fonts.gstatic.com/"><link rel="stylesheet" href="index_files//css2"></head>

   <body data-new-gr-c-s-check-loaded="14.1086.0" data-gr-ext-installed="">
    <header>
      <div id="logo-area" uk-grid="" class="uk-grid">
  		 <div class="uk-width-3-5 uk-first-column">
     		 <div><a href="https://openvrlab.github.io/index.html" class="uk-logo"> Open Access Tools and Libraries for Virtual Reality - 3nd Edition</a></div>
        <div id="citation">IEEE VR 2024 Workshop - Open Access Tools and Libraries for Virtual Reality<br/>  <br/></div>
      </div>
     
    </div></header>

   
		<br/>
		<br/>
   	<div id="page-container" class="full-viewport uk-grid-row-large uk-grid-collapse uk-grid" uk-grid="">
      <div class="leftcol uk-width-1-5 uk-first-column"></div>

		<br/>
		<br/>
     
	<div class="middlecol uk-width-4-5">
      

		
         <div class="workshop-title">Open Access Tools (OAT) and Libraries for Virtual Reality</div>


<a href="index23.html">OAT workshop 2023, Shanghai, China</a><br/>
<a href="index22.html">OAT workshop 2022, New Zeeland </a> <br/>

        <br/>

        <div class="workshop-subsection">Workshop Overview</div>
        <p>Virtual reality researchers and developers need tools to develop state of the art technologies that will help advance knowledge. 
		Promoting open-Source tools which can be modified or redistributed will be of great help for doing this. 
		Open-Source tools are one means to propagate best practice as they lower the barrier to entry for researchers from an engineering point of view, 
		while also embodying the prior work on which the tools were built. Open access tools are also critical to eliminate redundancies 
		and increase world research collaboration in VR. At a time that academic research is growing it also needs to move as fast as industry, 
		collaboration and shared tools are the best way to do it. In this scenario, it is more important than ever that academic research builds 
		upon best practices and results to amplify impact in the broader field. 
		</p>

         <div class="workshop-subsection">Description</div>
         <p>
		Driven by uptake in consumer and professional markets, virtual reality technologies are now being developed at a significant pace. 
		 Open access tools are also critical to eliminate redundancies and increase world research collaboration in VR. At a time that academic 
		 research needs to move as fast as the industry, collaboration and shared tools are the best way to do it.  
		We will gather creators and users of open-access libraries ranging from avatars (like the Microsoft Rocketbox avatar library) to 
		 animation to networking (Ubiq toolkit) to explore how open access tools are helping advance the VR community.  
		 We invite all researchers to join the second edition of this workshop and learn from existing libraries. 
		 We also invite submissions on technical and systems papers that describe new libraries and or new features of existing libraries. 
		 In this workshop, we will explore these open-access tools, their accessibility, and what type of applications they enable. 
			 
		</p>

		<p> 
		We invite all researchers to join the second edition of this workshop and learn from existing libraries.
			We also invite submissions on technical and systems papers that describe new libraries and or new features on existing libraries. 
			In this workshop, we will explore these open-access tools, their accessibility, and what type of applications they enable. 
			Finally, we invite users of current libraries to submit their papers and share their learnings while using these tools. 
			All papers and repos will be collected and curated in the workshop website  https://openaccess.github.io/
		</p>



	<div id="callforpapers">

		  <div class="workshop-subsection">Workshop topics</div>
	          <ul>
	            <li>Open source libraries and tools (from avatars to AI to tracking to networking)</li>
	            <li>Research libraries and tools</li>
	            <li>New tools, new features, new repos</li>
	            <li>Usage of libraries/tools</li>
	            <li>Open datasets</li>  
	        </ul>
		
	         <div class="workshop-subsection">Format and submission guidelines</div>
		         <p>Submissions should include a title, a list of authors, and be 2-4 pages. All paper submissions must be in English. 
				 All IEEE VR Conference Paper submissions must be prepared in IEEE Computer Society VGTC format 
				 (<a href="https://tc.computer.org/vgtc/publications/conference/"> https://tc.computer.org/vgtc/publications/conference/</a>) and submitted in PDF format. 
		           Accepting work of Research papers, Technical notes, Position papers, Work-in-progress papers.</p>
		
		         <p>The accepted papers will be featured in the IEEE Xplore library. Additional pages can be considered on a case by case basis, 
				 but you should check with the workshop organizers
				 before the submission deadline. Acceptable paper types are work-in-progress, research papers, position papers, or commentaries. Submissions will be reviewed by the organisers and accepted submissions will give a 10-minute talk with a panel discussion at the end of the session. At least one author must register for the workshop. Selected submissions will get the opportunity to be extended to articles to be considered for a special issue.</p>
				<p>To submit your work, visit <a href="https://new.precisionconference.com/vr">https://new.precisionconference.com/vr</a></p>
				The organizers will review all the submissions.
		<br/>
	        <div class="workshop-subsection">Important dates</div>
		        <ul>
		          <li>Submission deadline:&nbsp;January 12th, 2024&nbsp; &nbsp; </li>
		          <li>Notification of acceptance:&nbsp;January 18th, 2024 </li>
		          <li>Camera-ready deadline:&nbsp;January 24th, 2024 </li>
		          <li>The workshop will be in person.</li>
		        </ul>
	</div>
	      
        <div class="workshop-subsection">Workshop agenda - March 16 (Saturday) or 17 (Sunday), 2024. (TBD)</div>
	        <ul>
	          <li>8:40-8:55: Introduction by the organizers </li>
	          <li>9:00-10:00: Keynote Talk and Discussion</li>
     		  <li>10:00-11:00: Presentations of the accepted papers: 10 min per submission + 5 minutes discussion</li>
			  <li>11:00-11:15: Intermission</li>	
	          <li>11:15-12:15: Presentations of the accepted papers: 10 min per submission + 5 minutes discussion</li>
	          <li>12:15-12:30: Closing and call for getting involved in Open Sourcing efforts</li>				
		    </ul>

         <div id="organizers">
           <div class="workshop-subsection">Workshop organizers</div>
             <div class="organizers uk-grid-row-medium uk-grid" uk-grid="">
				 
	            <div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img width="200" src="index_files/matias.jpeg"></div>
	               <div class="middlecol uk-width-3-4 uk-grid-margin">
	                 <h5>Matias Volonte, Clemson University<br/> mvolont@clemson.edu</h5>
	                 <p>Matias Volonte is  an Assistant Professor  at Clemson University. Currently, Matias is investigating the use of virtual agents in which human-agent 
				 relationships improve task outcomes in the healthcare domain. Matias is interested in researching simulated face-to-face conversations with an 
				 emphasis on the relational and conversational aspects of these interactions. In his Ph.D. at Clemson University,</p>
	               </div> 
		     
	            <div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img width="200" src="./index_files/asteed.jpeg"></div>
		            <div class="middlecol uk-width-3-4 uk-grid-margin">
		              <h5>Anthony Steed, University College London <br/>a.steed@ucl.ac.uk @anthony_steed</h5>
		              <p>Prof. Anthony Steed is Head of the Virtual Environments and Computer Graphics (VECG) group at University College London. 
				      Prof. Steed’s research interests extend from virtual reality systems, through to mobile mixed-reality systems, 
				      and from system development through to measures of user response to virtual content. He has worked extensively on systems for collaborative mixed reality. 
				      He is lead author of a review “Networked Graphics: Building Networked Graphics and Networked Games”. He was the recipient of the IEEE VGTC’s 2016 Virtual 
				      Reality Technical Achievement Award.</p>
		            </div>		
		   <div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img width="200" src="index_files/roshan.jpg"></div>
	               <div class="middlecol uk-width-3-4 uk-grid-margin">
	                 <h5>Roshan Venkatakrishnan, University of Florida <br/>rvenkatakrishnan@ufl.edu</h5>
	           <p>Roshan Venkatakrishnan is a Research Associate in the Department of Computer and Information Science and Engineering at the University of Florida. 
			   He uses a human-centered approach leveraging virtual, augmented, and mixed reality environments to understand human perception and action. 
			   Roshan holds a Ph.D. in Human-Centered Computing from Clemson University where he specialized in researching user representations and their 
			   effects on the perception of action capabilities in the near field. Broadly speaking, his research interests spans topics including self-avatars and embodiment; 
			   depth, auditory, size, and affordance perception; and cybersickness in these immersive mediums.</p>
	          </div>	
	           <div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img width="200" src="index_files/bala.png"></div>
	               <div class="middlecol uk-width-3-4 uk-grid-margin">
	                 <h5>Bala Kumaravel, Microsoft&nbsp;&nbsp;<br/>
	                 bala.kumaravel@microsoft.com</h5>
	                 <p>He is a senior researcher at Microsoft Research, Redmond at the EPIC team(opens in new tab). He works on leveraging Generative AI models (LLMs and Diffusion models) 
				 to enhance user productivity and collaboration in business-critical applications. He is particularly interested in customizing, finetuning, 
				 and aligning generative AI models for specific end-user applications.&nbsp;Before joining Microsoft, he completed my Ph.D. at the University of California, Berkeley. </p>
	               </div>       			

		       <div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img width="200" src="index_files/hasti.PNG"></div>
	               <div class="middlecol uk-width-3-4 uk-grid-margin">
	                 <h5>Hasti Seifi, Arizona State University <br/>Hasti.Seifi@asu.edu</h5>
		           <p>Dr. Seifi is an assistant professor in the School of Computing and Augmented Intelligence at Arizona State University. Previously, she was at the University of Copenhagen
				   and Max Planck Institute for Intelligent Systems. Dr. Hasti aims to democratize access to emerging technologies such as haptics, VR/AR, and robotics. 
				   She has helped create open-source datasets, interactive visualizations, and educational content for VR, haptics, and physical HRI such as LocomotionVault, 
				   Haptiedia, VibViz, LearnHaptics, and RobotHand
		           </p>
	               </div>
		     
	           <div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img width="200" src="index_files/eric2.jpg"></div>
	               <div class="middlecol uk-width-3-4 uk-grid-margin">
	                 <h5>Eric Gonzalez, Google<br/>
	                 ejgonz@google.com</h5>
	                 <p>He is a Ph.D. student in Human-Computer Interaction at Stanford, working with Sean Follmer in the SHAPE Lab. His research interests are primarily in the 
				 fields of haptics and AR/VR. He is interested in designing systems that enrich users’ physical interactions with digital media, as well as studying 
				 how these systems impact and are influenced by human perception. His research interests are primarily in the fields of haptics and AR/V and    
				 designing systems that enrich users’ physical interactions with digital media, as well as studying how these systems impact and are influenced by human perception. </p>
	                
	               </div>  
	        
	    	         <div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img width="200" src="index_files/yuhang.PNG"></div>
	               <div class="middlecol uk-width-3-4 uk-grid-margin">
			        <h5>Yuhang Zhao, University of Wisconsin-Madison <br>yuhang.zhao@cs.wisc.edu</h5>
		            <p>Dr. Zhao is an assistant professor in the department of Computer Sciences at the University of Wisconsin-Madison. She received her Ph.D. degree from Cornell University. Her research interests include Human-Computer Interaction, accessibility, augmented and virtual reality, and mobile interaction. She designs systems and interaction techniques to empower people with diverse abilities both in real-life and virtual worlds.
		             </p>
		           </div> 
	         <div class="leftcol uk-width-1-4 uk-grid-margin uk-first-column"><img width="200" src="index_files/mar.jpeg"></div>
	               <div class="middlecol uk-width-3-4 uk-grid-margin">
		                 <h5>Mar Gonzalez Franco, Google <br/> margon@google.com  </h5>
		                 <p>Mar is a prolific computer scientist and 2022 IEEE VR New researcher awardee.
					 She is deeply involved with creating Open Source tools, and has released the Microsoft Rocketbox avatars, the Movebox and Headbox animation toolkits, Embodiment questionnaires, 
					 mods for the AIRSim simulation tool to do crowds (CityLifeSim), code/HW for latency measurement, and more recently AI tools to do segmentation like Google DiffSeg. 
					 (all repos are available in https://github.com/margonzalezfranco/) 
		                 </p>
		           </div> 	
	 </div>	     
 </div>	

<br/><br/><br/>
	<div id="organizers" >	 
	        <div class="workshop-subsection"><b>IEEE VR 2023 Workshop submissions</b> </div>
		<p>Inceptor: An Open Source Tool for Automated Creation of 3D Social Scenarios
		<br/>
		 Dan Pollak,&nbsp; School of Computer Science, RUNI
	           <br/>
	          Jonathan Giron,&nbsp; &nbsp;School of Computer Science, RUNI
			   <br/>
	          Doron Friedman,&nbsp; &nbsp;School of Computer Science, RUNI	
			</p>
			 <a href="https://drive.google.com/file/d/12zM-OIQh6LMDtoISG-p2vCtHAQzSKkLw/view?usp=sharing" target="_blank">
				 <img width="200" src="index_files/papers/inceptor.png"><br/>pdf </a><br/> 
		
		<br/>
		
	           <p>Cybersickness assessment framework(CSAF): An Open Source Repository for Standardized Cybersickness Experiments
		Adriano Viegas Milani,&nbsp; Ecole Polytechnique Federale de Lausanne
	           <br/>
	          Nana Tian&nbsp; &nbsp;Ecole Polytechnique Federale de Lausanne
		</p>
			   <a href="https://drive.google.com/file/d/16a4rWTT1klpUkHGhUKy_3GE644gkenBP/view?usp=sharing" target="_blank"><img width="200" src="index_files/papers/cyber.png"><br/>pdf </a>
	          		        			
		
	          <p>Visualsickness: A web application to record and organize cybersickness data 
			<br/>
	          Elliot Topper, &nbsp; The College of New Jersey
	           <br/>
	         Paula Arroyave,&nbsp; &nbsp;The College of New Jersey
			   <br/>
	          Sharif Mohammad Shahnewaz Ferdous,&nbsp; &nbsp;The College of New Jersey</p>
			  <a href="https://drive.google.com/file/d/1upo7VZTTEYI8H-2gzSeIGuErmvaKvyt9/view?usp=sharing" target="_blank"> <img width="200" src="index_files/papers/visualSickness.png"/><br/>pdf </a>
			
	           <p>Streamlining Physiological Observations in Immersive Virtual Reality Studies with the Virtual Reality Scientific Toolkit
			
				<br/>
	          Jonas Deuchler,&nbsp; Hochschule Karlsruhe, University of Hohenheim
	           <br/>
	          Wladimir Hettmann,&nbsp; &nbsp;Hochschule Karlsruhe, University of Hohenheim
			   <br/>
	          Daniel Hepperle,&nbsp; &nbsp;Hochschule Karlsruhe, University of Hohenheim
			   <br/>
	          Matthias Wolfel,&nbsp; &nbsp;Hochschule Karlsruhe, University of Hohenheim			
 </p>
  <a href="https://drive.google.com/file/d/1dObUq8CQoMltrse54rGOo0InP4ngKS-l/view?usp=sharing" target="_blank"> <img width="200" src="index_files/papers/streamlining.png"/><br/>pdf </a>
		
	           <p>Semi-Automatic Construction of Virtual Reality Environment for Highway Work Zone Training using Open-Source Tools
					<br/>
	          Can Li,&nbsp;University of Missouri
	           <br/>
	          Zhu Qing,&nbsp; &nbsp;University of Missouri
			   <br/>
	          Praveen Edara,&nbsp; &nbsp;University of Missouri		
			   <br/>
	          Carlos Sun,&nbsp; &nbsp;University of Missouri	
			   <br/>
	          Bimal Balakrishnan,&nbsp; &nbsp;Mississippi State University	
			   <br/>
	          Yi Shang,&nbsp; &nbsp;University of Missouri					
			</p>
			<a href="https://drive.google.com/file/d/1COFrAhT9SARVEAjYWiQLnAOaQZJC2BTo/view?usp=sharing" target="_blank"><img width="200" src="index_files/papers/streamline.png"><br/>pdf </a>

		
	           <p>A Preliminary Interview: Understanding XR Developers’ Needs towards Open-Source Accessibility Support <br/>
			  
			<p>Tiger F. Ji,&nbsp; University of Wisconsin-Madison
	           <br/>
	          Yaxin Hu,&nbsp; &nbsp;School of Computer Science, RUNI
			   <br/>
	          Yu Huang,&nbsp; &nbsp;Vanderbilt University	
			   <br/>
	          Ruofei Du,&nbsp; &nbsp;Google Labs		
			   <br/>
	          Yuhang Zhao,&nbsp; &nbsp;University of Wisconsin-Madison </p>	
			<a href="https://drive.google.com/file/d/13pJKQFQCrWohfl0aEGl1nLqb0H4Ibxdc/view?usp=sharing" target="_blank"><img width="200" src="index_files/papers/xr.png"><br/>pdf </a>

				
	           <p>Ubiq-Genie: Leveraging External Frameworks for Enhanced Social VR Experiences <br/>
	          Nels Numan,&nbsp; &nbsp;University College London
			   <br/>
	          Daniele Giunchi,&nbsp; &nbsp;University College London	
			   <br/>
	          Benjamin Congdon,&nbsp; &nbsp;University College London		
			   <br/>
	          Anthony Steed,&nbsp; &nbsp;University College London		</p>										
			 
			   <a href="https://drive.google.com/file/d/18ZuAvi3LXPCjiPnWqWye2r8_nC3Ncdbd/view?usp=sharing" target="_blank"><img width="200" src="index_files/papers/conv.png"> pdf <br/></a>

				
	           <p>Virtual-to-Physical Surface Alignment and Refinement Techniques for Handwriting, Sketching, and Selection in XR<br/>
	          Florian Kern,&nbsp; &nbsp;HCI Group, University of Wurzburg
			   <br/>
	          Jonathan Tschanter,&nbsp; &nbsp;HCI Group, University of Wurzburg	
			   <br/>
	          Marc Erich Latoschik,&nbsp; &nbsp;HCI Group, University of Wurzburg													
			</p>
			<a href="https://drive.google.com/file/d/13f7yf_JuIFGEtOhJ5O0Qc5-y6Hwuo0uK/view?usp=sharing" target="_blank"> <img width="200" src="index_files/papers/surface.png"> pdf <br/></a>		
			
	
	<br/>
	
	<br/>
	
	<br/>


	<div class="workshop-subsection"><b>IEEE VR 2022 Workshop Submissions</b></div>
		          	 		 				
		<br/>
		
		<br/>
		  
		<b>Data Capture, Analysis and Understanding </b>
		
		
		<p ><b> </b>Rag-Rug: An Open Source Toolkit for
		Situated Visualization and Analytics 
		<br/>
		Dieter Schmalstieg, Philipp Fleck
		(Invited from TVCG paper) 
			</p>
			 <iframe width="300" height="169" src="https://www.youtube.com/embed/mFxSdvQhSVU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			<br/> <a href="https://github.com/philfleck/ragrug">https://github.com/philfleck/ragrug</a>
		<br/>
		 <p ><b> </b>Developing Mixed Reality Applications with Platform for
		Situated Intelligence <br/>
		Sean Andrist, Dan Bohus, Ashley Feniello, Nick Saw 
			</p>
		<iframe width="300" height="169" src="https://www.youtube.com/embed/Gfs808q8S0I" title="YouTube video player"  frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			<br/> <a href="https://github.com/microsoft/psi">https://github.com/microsoft/psi</a>
		<br/>
		<p ><b></b>STAG: A Tool for realtime Replay
		and Analysis of Spatial Trajectory and Gaze Information captured in Immersive
		Environments <br/> Aryabrata Basu
			</p>
		<iframe width="300" height="169" src="https://www.youtube.com/embed/l8d1t6oncew" title="YouTube video player"  frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		
		<p ><b></b>Excite-O-Meter: an Open-Source
		Unity Plugin to Analyze Heart Activity and Movement Trajectories in Custom VR
		Environments <br/> 
		Luis Quintero, Panagiotis Papapetrou, John
		Edison Muñoz Cardona, Jeroen de De
		mooij, Michael Gaebler
			</p>
			 <iframe width="300" height="169" src="https://www.youtube.com/embed/zQRiGuwbXI0" title="YouTube video player"  frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			<br/> <a href="https://github.com/luiseduve/exciteometer/">https://github.com/luiseduve/exciteometer/</a>
		<br/>
			 
		 
		<br/>
		<p ><b> Perception and Cognition</b>
		<br/></p>
		
		 <p ><b></b>An Open Platform for Research about Cognitive Load in
		Virtual Reality <br/>
		Olivier Augereau, Gabriel Brocheton, Pedro
		Paulo Do Prado Neto</p>
			<img  width="300" src="https://git.enib.fr/g7broche/vr-ppe/-/raw/master/Instructions/Images/Screen_TripleTask1.png"/>
			 <a href="https://git.enib.fr/g7broche/vr-ppe">https://git.enib.fr/g7broche/vr-ppe</a>
		 
		 <br/>
		<p ><b></b>Human Vision vs. Computer Vision: A Readability Study in a
		Virtual Reality Environment  <br/>
		Zhu Qing, Praveen Edara</p>
		 <img  width="300" src="./index_files/vision.JPG"/>
		
		<p ><b> </b>Asymmetric Normalization in Social Virtual Reality Studies 
		 <br/>
		Jonas Deuchler, Daniel Hepperle, Matthias
		Wölfel</p>
				<img  width="300" src="./index_files/asymetric.JPG" / >
							      
				 <a href="https://github.com/ixperience-lab/VRSTK">https://github.com/ixperience-lab/VRSTK</a>
		 				      
		
		<br/>  
		
		<p ><b>Authoring and Access</b>  
		<br/></p>
		 
		<p ><b> </b>BabiaXR: Virtual Reality software
		data visualizations for the Web
		 <br/>
		David Moreno-Lumbreras,
		Jesus M. Gonzalez-Barahona, Andrea Villaverde</p>
		<a href="https://babiaxr.gitlab.io/aframe-babia-components/examples/demos/1.0.7/">
		<img   width="300" src="https://babiaxr.gitlab.io/aframe-babia-components/tests/screenshots/other_snapshots.js/demo_1.0.7.png" 
		/></a>
		 <a href="https://babiaxr.gitlab.io/">https://babiaxr.gitlab.io/</a>
		
		<p ><b></b>RealityFlow: Open-Source
		Multi-User Immersive Authoring 
		 <br/>
		John T Murray</p>
																		
		 <img  width="300" src="./index_files/realityflow.JPG"/>
			 <a href="https://github.com/many-realities-studio/realityflow">https://github.com/many-realities-studio/realityflow</a>
		 
		
		<p ><b></b>NUI-SpatialMarkers: AR Spatial
		Markers For the Rest of Us 
		 <br/>
		Alex G Karduna, Adam Sinclair
		Williams, Francisco Raul Ortega</p>
		 <a href=" https://github.com/NuiLab/NUI-SpatialMarkers"> https://github.com/NuiLab/NUI-SpatialMarkers</a>
		
		<br/>
		<p ><b>Avatar Tools</b>
		<br/></p>
		
		
		<p ><b> </b>Physics-based character animation for Virtual Reality 
		 <br/> Joan Llobera, Caecilia Charbonnier</p>
		<img   width="350" src="https://joanllobera.github.io/marathon-envs/images/MarathonEnvsBanner.gif" 
		/><a href="https://joanllobera.github.io/marathon-envs/">https://joanllobera.github.io/marathon-envs/</a>	 
								      
		<p ><b></b>HeadBox: A Facial Blendshape Animation Toolkit for the Microsoft Rocketbox Library 
		 <br/> Matias Volonte, Eyal
		Ofek, Ken Jakubzak, Shawn Bruner, Mar Gonzalez-Franco</p>
		<iframe width="300" height="169" src="https://www.youtube.com/embed/hgUGOjc6hOg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
			<br/><a href="https://github.com/openVRlab/Headbox">https://github.com/openVRlab/Headbox</a>	 
			 
		<p><b></b>Integrating Rocketbox Avatars with
		the Ubiq Social VR platform 
		 <br/> 
		Lisa Izzouzi, Anthony Steed</p>
		
		<iframe width="300" height="169" src="https://www.youtube.com/embed/kLqhsbTd4Zo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
		
		<br/>
		<a href="https://github.com/UCL-VR/ubiq">https://github.com/UCL-VR/ubiq</a>
									  
		

      </div>
		
		  
	   
		      
    <div id="footer-area" class="uk-grid-collapse uk-grid uk-grid-margin uk-first-column" uk-grid="">
          <div class="leftcol uk-width-1-5 uk-first-column"></div>
          <div class="middlecol uk-width-3-5">For questions and comments, please contact <a href="mailto:m.volonte@northeastern.edu">Matias Volonte</a>.</div>
          <div class="rightcol uk-width-1-5"></div>
    </div>

   

</div>
</div></div></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>
